
\section{Introduction}

Parser combinators have made functional languages such as Haskell
shine. They are a prime example of the advantages Embedded Domain
Specific Languages~\cite{hudak1996building} provide the end user. She not only has access
to a set of powerful and composable abstractions but she is also
able to rely on the host language's existing tooling and libraries.
She can get feedback from the static analyses built in the compiler
(e.g. type and coverage checking) and can exploit the expressivity
of the host language to write generic parsers thanks to polymorphism
and higher order functions.

However she only gets the guarantees the host language is willing
to give. In non-total programming languages such as Haskell this
means she will not be prevented from writing parsers which will
unexpectedly fail on some (or even all!) inputs. Handling a
left-recursive grammar is perhaps the most iconic pitfall leading
beginners to their doom: a parser never making any progress.
Other issues one may want guarantees about range from unambiguity
to complexity with respect to the input's size.

We start with a primer on parser combinators and follow up with the
definition of a broken parser which is silently accepted by Haskell.
We then move on to Agda~\cite{norell2009dependently} and introduce combinators to
define functions by well-founded recursion. This allows us to define
a more informative notion of parser and give more precise types to
the combinators commonly used. We then demonstrate that broken parsers
such as the one presented earlier are rejected whilst typical example
can be ported with minimal modifications.

\remark{Agda-centric} Although we do use some Agda-specific techniques
in order to have a codebase as idiomatic as possible, we do not expect
the reader to be well-versed in them. We insert remarks similar to this
one throughout the paper to clarify confusing points, and give pointers
to more in-depth explanations to the interested reader.

This work is however not limited to Agda: it can be ported to other
dependently-typed languages and we have already done so for Coq~\cite{Coq:manual}
(\url{https://github.com/gallais/parseque}) and
Idris~\cite{brady2013idris} (\url{https://github.com/gallais/idris-tparsec}).


\section{A Primer on Parser Combinators}

\subsection{Parser Type}

Let us start by reminding ourselves what a parser is. Although we
will eventually move to a more generic type, Fritz Ruehr's rhyme
gives us the \emph{essence} of parsers:

\begin{verse}
  A Parser for Things\\
  is a function from Strings\\
  to Lists of Pairs\\
  of Things and Strings!
\end{verse}

This stanza translates to the following Haskell type. We use a \textbf{newtype}
wrapper to have cleaner error messages:

\shgrab{parser}

It is naturally possible to run such a parser and try to extract
a value from a valid run. Opinions may vary on the exact definition
of a successful parse: should a run with leftover characters or an
ambiguous result be accepted? We decide against both in the following
code snippet but it is not a crucial point.

\shgrab{parse}

Once we are equipped with this type of parsers and a function to run
them, we can start providing some examples of parsers and parser combinators.

\subsection{(Strongly-Typed) Combinators}

The most basic parser is the one that accepts any character.
It succeeds as long as the input string is non empty and
returns one result: the tail of the string together with the
character it just read.

\shgrab{anychar}

However what makes parsers interesting is that they recognize
structure. As such, they need to reject invalid inputs. The
parser only accepting decimal digits is a bare bones example.
It can be implemented in terms of \parser{guard}, a higher
order parser checking that the value returned by its argument
abides by a predicate which can easily be implemented using
functions from the standard library.

\shgrab{guard}
\hgrab{digit}

These two definitions are only somewhat satisfactory: the
result of the \parser{digit} parser is still \emph{stringly-typed}.
Instead of using a predicate to decide whether to keep the
value, we can opt for a validation function of type
\mbox{\textit{a → Maybe b}} which returns a witness whenever the
check succeeds. To define this refined version of \parser{guard}
called \parser{guardM} we can again rely on the standard library:

\shgrab{guard2}

\begin{itemize}
  \item \textit{traverse f} of type \mbox{\textit{(String, a) → Maybe (String, b)}}
    takes apart a pair, applies $f$ to the second component and rebuilds the pair
    \emph{under} the \textit{Maybe} type constructor,
  \item \textit{fmap} applies this function to all the elements in the list obtained
    by running the parser $p$,
  \item and \textit{catMaybes} of type \mbox{\textit{[Maybe (String, b)] → [(String, b)]}}
    only retains the values which successfully passed the test.
\end{itemize}

In our concrete example of recognizing a digit, we want to return the
corresponding \type{Int}. Once more the standard library has just the
right function to use together with \parser{guardM}: \textit{readMaybe}
of (specialised) type \mbox{\textit{String → Maybe Int}}.

\shgrab{digit2}


\subsection{Expressivity: Structures, Higher Order Parsers and Fixpoints}

%\subsection{Code Reuse: Recognizing Structures}

We have seen how we can already rely on the standard library
of the host language to seamlessly implement combinators.
We can leverage even more of the existing codebase by noticing
that the type constructor \type{Parser} is a \type{Functor},
an \type{Applicative}~\cite{mcbride2008applicative}, a \type{Monad}
and also an \type{Alternative}.

\textbf{Functor} means that given a function of the right type, we
can alter the values returned by a parser. That is, we have a function
(which corresponds to the infix combinators (<\$>)):

\shgrab{fmap}

\textbf{Applicative} means two things. First, that given a value of type
\type{a}, we can define a parser for values of type \type{a}. Second, that
given a parser for a function and a parser for its argument we can run both
and apply the function to its argument. That is, we have two functions:

\shgrab{applicative}

\textbf{Monad} means that we are entitled to inspect the result of a
first parser to decide which one to run next. This brings us beyond the
realm of context-free grammars. That translates into the existence of
one function:

\shgrab{monad}

\textbf{Alternative} means that for all type \type{a}, \type{Parser a}
forms a monoid. It allows us to take the disjunction of various parsers,
the failure of one leading to the next being used.

\shgrab{alternative}



%\subsection{Expressivity: Higher Order Parsers and Fixpoints}

Our first example of a higher order parser was \parser{guard}
which takes as arguments a validation function as well as another
parser and produces a parser for the type of witnesses returned
by the validation function.

The two parsers \AB{some} and \AB{many} turn a parser for elements
into ones for non-empty and potentially empty lists of such elements
respectively. They concisely showcase the power of mutual recursion,
higher-order functions and the \type{Functor}, \type{Applicative},
and \type{Alternative} structure.
\medskip{}

\begin{minipage}{0.45\textwidth}
\hgrab{some}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\hgrab{many}
\end{minipage}

\remark{Non-Commutative} The disjunction combinator is non-commutative
as ultimately we obtain a list (and not a set) of possible results. As
such the definitions of \textit{some} and \textit{many} will try to
produce the longest list possible as opposed to a flipped version of
\text{many} which would start by returning the empty list and slowly
offer longer and longer matches.

\section{The Issue with Haskell's Parser Types}\label{sec:haskellproblem}

The ability to parse recursive grammars by simply declaring
them in a recursive manner is however dangerous: unlike type
errors which are caught by the typechecker and partial covers
in pattern matchings which are detected by the coverage checker,
termination is not guaranteed.

The problem already shows up in the definition of \parser{some}
which will only make progress if its argument actually uses up
part of the input string. Otherwise it may loop. However this
is not the typical hurdle programmers stumble upon: demanding
a non empty list of nothing at all is after all rather silly.
The issue manifests itself naturally whenever defining a left
recursive grammar which leads us to introducing the prototypical
such example: \type{Expr}, a minimal language of arithmetic
expressions.

\[
\texttt{Expr} ~::=~ \texttt{<Int>} ~|~ \texttt{<Expr>} ~\text{`+`}~ \texttt{<Expr>}
\]

The intuitive solution is to simply reproduce this definition by
introducing an inductive type for \type{Expr} and then defining
the parser as an alternative between a literal on one hand and a
sub-expression, the character '+', and another sub-expression on
the other.

\shgrab{Expr}
\hgrab{expr}

However this leads to an infinite loop. Indeed, the second
alternative performs a recursive call to \parser{expr} even
though it hasn't consumed any character from the input string.

The typical solution to this problem is to introduce two 'tiers'
of expressions: the \parser{base} ones which can only be whole
expressions if we consume an opening parenthesis first and the
\parser{expr} ones which are left-associated chains of \parser{base}
expressions connected by '+'.\label{parser:expr}

\shgrab{base}
\hgrab{expr2}

This presentation is still sub-optimal; users would traditionally
be encouraged to used combinators such as \textit{chainl} to avoid
this issue. We will only discuss later in Section~\ref{sec:leftchains}.

This approach can be generalised when defining more complex
languages by having even more tiers, one for each
\emph{precedence level}, see for instance Section~\ref{example:expr}.
An extended language of arithmetic expressions would for
instance distinguish the level at which addition and
subtraction live from the one at which multiplication and
division do.

Our issue with this solution is twofold. First, although we did
eventually manage to build a parser that worked as expected,
the compiler was unable to warn us and guide us towards this
correct solution. Additionally, the blatant partiality of
some of these definitions means that these combinators and
these types are wholly unsuitable in a total setting. We could, of
course use an escape hatch and implement our parsers in Haskell
but that would both be unsafe and mean we would not be able
to run them at typechecking time which we may want to do if
we embed checked examples in our software's documentation,
or use compilation-time configuration via e.g. dependent type
providers~\cite{christiansen2013dependent}.

\section{Indexed Sets and Course-of-Values Recursion}

Our implementation of Total Parser Combinators is in Agda,
a total dependently typed programming language and it will
rely heavily on indexed sets. But the indices will not be
playing any interesting role apart from enforcing totality.
As a consequence, we introduce combinators to build indexed
sets without having to mention the index explicitly. This
ought to make the types more readable by focusing on the
important components and hiding away the artefacts of the
encoding.

The first kind of combinators corresponds to operations on
sets which are lifted to indexed sets by silently propagating
the index. We only show the ones we will use in this paper:
the pointwise arrow and product types and the constant function.
The second kind of combinator corresponds to universal quantification:
it turns an indexed set into a set.

\begin{minipage}{0.55\textwidth}
  \agrab{Relation/Unary/Indexed}{arrow}
  \agrab{Relation/Unary/Indexed}{product}
\end{minipage}
\begin{minipage}{0.30\textwidth}
  \agrab{Relation/Unary/Indexed}{constant}
  \agrab{Relation/Unary/Indexed}{forall}
\end{minipage}

\remark{Mixfix Operators} In Agda underscores correspond to
positions in which arguments are to be inserted. It may be a
bit surprising to see infix notations for functions taking
three arguments but they are only meant to be partially applied.
\remark{Implicit Arguments} We use curly braces so that the
index we use is an \emph{implicit} argument we will never
have to write: Agda will fill it in for us by unification.
\medskip{}

We can already see the benefits of these aliases. For instance
the fairly compact expression \AF{[} (\AF{κ} \AB{P} \AF{⊗} \AB{Q}) \AF{⟶} \AB{R} \AF{]}
corresponds to the more verbose type
\AS{∀} \{\AB{n}\} \AS{→} (\AB{P} \AF{×} \AB{Q} \AB{n}) \AS{→} \AB{R} \AB{n}.


Last but not least, we introduce a type constructor which takes a
\AD{ℕ}-indexed set and produces the set of valid recursive calls
for a function defined by course-of-values recursion. By analogy
to modal logic we call it \BOX{} after the ``necessity'' modality
whose interpretation in a Kripke semantics is eerily similar to
our type constructor.

\sagrab{Induction/Nat/Strong}{box}

\remark{Record Wrapper} Instead of defining \BOX{} as a
function like the other combinators, we wrap the
function space in a record type. This prevents normalisation from
unfolding the combinator too eagerly and makes types more readable
during interactive development.

\remark{Irrelevance} The argument stating that \AB{m} is strictly
smaller than \AB{n} is preceded by a dot. In Agda, it means that
this value is irrelevant and can be erased by the compiler.
In Coq, we would define the relation \AF{\_<\_} in \type{Prop} to
achieve the same.
\medskip{}

This construct can also be understood as analogous to the later modality
showing up in Guarded Type Theory~\cite{vezzosi2015guarded}. It empowers
the user to give precise types in a total language to programs commonly
written in partial ones (see e.g. the definition of \textit{fix} below).
The first thing we can notice is the fact that \textbf{\BOX{} is a
functor}; that is to say that given a natural transformation from
\AB{A} to \AB{B}, we can define a natural transformation from
\BOX{} \AB{A} to \BOX{} \AB{B}.

\sagrab{Induction/Nat/Strong}{map}

\remark{Copatterns} The definition of \AF{map} uses
the \BOX{} field named \ARF{call} on the \emph{left hand side}.
This is a copattern~\cite{abel2013copatterns}, meaning that we
explain how the definition is \emph{observed} (via \ARF{call})
rather than \emph{constructed} (via \AIC{mkBox}).
\medskip{}

Because less than (\AF{\_<\_}) is defined in terms of less than
or equal (\AD{\_≤\_}), \AF{≤-refl} which is the proof that \AD{\_≤\_}
is reflexive is also a proof that any \AB{n} is strictly smaller than
\AN{1} \AF{+} \AB{n}. We can use this fact to write the following
\AF{extract} function:

\sagrab{Induction/Nat/Strong}{extract}

\remark{Counit} The careful reader will have noticed
that this is not quite the \textit{extract} we would expect from a
comonad: for a counit, we would need a natural transformation
between \BOX{} \AB{A} and \AB{A} i.e. a function of type
\mbox{\AF{[} \BOX{} \AB{A} \AF{⟶} \AB{A} \AF{]}}. We will not be able
to define such a function: \BOX{} \AB{A} \AN{0} is isomorphic to
the unit type so we would have to generate an \AB{A} \AN{0} out
of thin air. The types \AB{A} for which \BOX{} has a counit are
interesting in their own right: they are inhabited at every single
index as demonstrated by \AF{fix} later on.

\medskip{}

Even though we cannot have a counit, we are still able to define
a comultiplication thanks to the fact that \AF{\_<\_} is transitive.

\sagrab{Induction/Nat/Strong}{duplicate}

\remark{Identifiers in Agda} Any space-free string which
is not a reserved keyword is a valid identifier. As a consequence
we can pick suggestive names such as \AB{m<n} for a proof that $m < n$
(notice the extra spaces around the infix operator $(<)$).

\medskip{}

Exploring further the structure of the functor \BOX{}, we can observe
that just like it is not quite a comonad, it is not quite an applicative
functor. Indeed we can only define \AB{pure}, a natural transformation
of type \mbox{\AF{[} \AB{A} \AF{⟶} \AR{\BOX{}} \AB{A} \AF{]}}, for the types \AB{A}
that are downwards closed. Providing the user with \AB{app} is however
possible:

\agrab{Induction/Nat/Strong}{app}

Finally, we can reach what will serve as the backbone of our parser
definitions: a safe, total fixpoint combinator. It differs from the
traditional $Y$ combinator in that all the recursive calls have to be
guarded.

\sagrab{Induction/Nat/Strong}{fix}

If we were to unfold all the type-level combinators and record wrappers,
the type of \AF{fix} would correspond exactly to strong induction for
the natural numbers. Hence its implementation also follows the one of strong
induction: it is a combination of a call to \AF{extract} and an auxiliary
definition \AF{fix\BOX{}} of type \mbox{\AF{[} \AR{\BOX{}} \AB{A} \AF{⟶}
\AB{A} \AF{]} \AS{→} \AF{[} \AR{\BOX} \AB{A} \AF{]}}.

\remark{Generalisation}  A similar \BOX{}
type constructor can be defined for any induction principle relying
on an accessibility predicate. Which means that a library's types
can be cleaned up by using these combinators in any situation where
one had to give up structural induction for a more powerful alternative.

\section{Parsing, Totally}

As already highlighted in Section~\ref{sec:haskellproblem}, \AB{some}
and \AB{many} can yield diverging computations if the parser they are
given as an argument succeeds on the empty string. To avoid any such
issue, we adopt a radical solution: for a parser's run to be considered
successful, it must have consumed some of its input. Some nullability
can be recovered later (see Section~\ref{sec:failure}) when defining
combinators where one of the sub-parses is allowed to fail.

This can be made formal with the \AR{Success} record type: a \AF{Success}
of type \AB{A} and size {n} is a value of type \AB{A} together with the
leftovers of the input string of size strictly smaller than \AB{n}.

\agrab{Text/Parser/Success}{success}

\remark{Implicit Field} Like the arguments to a function can be implicit,
so can a record's fields. The user can leave them out when building a
value: they will be filled in by unification.
\medskip{}

Coming back to Fritz Ruehr's rhyme, we can define our own \AR{Parser}
type: a parser for things up to size \AB{n} is a function from strings
of length \AB{m} less or equal to \AB{n} to lists of \AR{Success}es of size \AB{m}.

\agrab{Text/Parser/Combinators}{parser}

\subsection{Our First Combinators}

Now that we have a precise definition of \AR{Parser}s, we can start
building our library of combinators. Our first example \AF{anyChar}
can be defined by copattern-matching and then case analysis on the
input string: if it is empty then the list of \AR{Success}es is also
empty, otherwise it contains exactly one element which corresponds to
the head of the input string and its tail as leftovers.

\agrab{Text/Parser/Combinators}{anyChar}

Unsurprisingly \AF{guardM} is still a valid higher-order combinator:
filtering out results which do not agree with a predicate is absolutely
compatible with the consumption constraint we have drawn. To implement
\AF{guardM} we can once more reuse existing library functions. Relying
this time on Agda's standard library rather than Haskell's, the set of
available function is slightly different. We use for instance \AF{gfilter}
which turns a \mbox{\AD{List} \AB{A}} into a \mbox{\AD{List} \AB{B}}
provided a predicate \mbox{\AB{A} \AS{→} \AD{Maybe} \AB{B}} and combine
\AF{sequence} and \AR{Success}'s \AF{map} to obtain a function akin to
\textit{traverse}.

\agrab{Text/Parser/Combinators}{guardM}

Demonstrating that \AR{Parser} is a functor goes along the same
lines: using \AD{List}'s and \AR{Success}'s \AF{map}s. Similarly,
we can prove that it is an \AF{Alternative}: failing corresponds
to returning the empty list no matter what whilst disjunction is
implemented using concatenation.

\agrab{Text/Parser/Combinators}{fmap}
\begin{minipage}{0.30\textwidth}
  \agrab{Text/Parser/Combinators}{fail}
\end{minipage}
\begin{minipage}{0.55\textwidth}
  \agrab{Text/Parser/Combinators}{disjunction}
\end{minipage}

So far the types we have ascribed to our combinators are, if we ignore
the $\mathbb{N}$ indices, exactly the
same as the ones one would find in any other parsec library. In none
of the previous combinators do we run a second parser on the leftovers
of a first one. All we do is either manipulate or combine the results
of one or more parsers run in parallel, potentially discarding some of
these results on the way.

However when we run a parser \emph{after} some of the input has already
been consumed, we could safely perform a \emph{guarded} call. This being
made explicit would be useful when using \AF{fix} to define a parser for
a recursive grammar. Luckily \AR{Parser} is, by definition, a
downwards-closed type. This means that we may use very precise types marking
all the guarded positions with \BOX{}; if the user doesn't need that extra
power she can very easily bypass the \BOX{} annotations by using \AF{box}:

\agrab{Text/Parser/Combinators}{box}

The most basic example we can give of such an annotation is probably the
definition of a conjunction combinator \AF{\_<\&>\_} taking two parsers,
running them sequentially and returning a pair of their results. The second
parser is given the type \mbox{\BOX{} \AR{Parser} \AB{B}} instead of
\mbox{\AR{Parser} \AB{B}} which we would expect to find in other parsec libraries.

\agrab{Text/Parser/Combinators}{conjunction}

We can immediately use all of these newly-defined combinators to give
a safe, total definition of \AF{some} which takes a parser for \AB{A}
and returns a parser for \AD{List$^+$} \AB{A}, the type of non-empty
lists of \AB{A}s. It is defined as a fixpoint and proceeds as follows:
it either combines a head and a non-empty tail using \mbox{\AF{$\_::^{+}\_$} :
  \AB{A} $\rightarrow$ \AD{List$^+$} \AB{A} $\rightarrow$
  \AD{List$^+$} \AB{A}} or returns a singleton list.

\agrab{Text/Parser/Combinators}{badsome}

\remark{Inefficiency} Unfortunately this definition is inefficient.
Indeed, in the base case \mbox{\AF{some} \AB{p}} is going to run the parser
\AB{p} twice: once in the first branch before realising that \AB{rec}
fails and once again in the second branch. Compare this definition to
the Haskell version (after inlining \AB{many}) where \AB{p} is run once
and then its result is either combined with a list obtained by recursion
or returned as a singleton:

\shgrab{some2}

\subsection{Failure is Sometimes an Option}\label{sec:failure}

This inefficiency can be fixed by introducing the notion of a potentially
failing sub-parse. We use the convention that \AF{?} marks the argument
of a combinator which is allowed to fail e.g. \AF{<\&?>} is the version
of the conjunction \AF{<\&>} whose second argument is allowed to fail
whilst \AF{<?\&>} may let its first argument do so. Which, in terms of
types, translates to:

\agrab{Text/Parser/Combinators}{conjunction2}

The $\textit{some}~\textit{p} \mathbin{<\!\!|\!\!>} \textit{pure}~ [\,]$ pattern
used in the definition of \textit{some} can be translated in our total
setting to a recursive call which is allowed to fail. This leads to the following
rethought definition of \mbox{\AF{some} \AB{p}}. The inefficiency of the previous
version has disappeared: \AB{p} is run once and dependening on the success
or failure of the recursive call it is either added to a non-empty list of
values or returned as a singleton.

\agrab{Text/Parser/Combinators}{goodsome}

\remark{Non-Compositional}\label{parser:some} The higher-order parser \AF{some}
expects a \emph{fully} defined parser as an argument. This makes
it impossible to use it as one of the building blocks of a larger,
recursive parser. Ideally we would rather have a combinator of
type \mbox{\AF{[} \AF{Parser} \AB{A} \AF{⟶} \AF{Parser} (\AF{List$^+$} \AB{A}) \AF{]}}.
This will be addressed in the next subsection.
\medskip{}

The potentially failing conjunction combinator \AF{\_<\&?>\_} can be
generalised to a more fundamental notion \AF{\_\&?>>=\_} which is a
combinator analogous to a monad's \textit{bind}. On top of running
two parsers sequentially (with the second one being chosen based on
the result obtained by running the first), it allows the second one to
fail and returns both results.

\agrab{Text/Parser/Combinators}{mbind}

These definitions make it possible to port a lot of the Haskell
definitions where one would use a parser which does not use any of its
input. Instead of encoding a potentially failing parse using the pattern
$\textit{p} \mathbin{<\!\!|\!\!>} \textit{pure}~ v$, we can explicitly
use a combinator acknowledging the authorized failure. And this is possible
without incurring any additional cost as the optimised version of \AF{some}
showed.

\subsection{Left Chains}\label{sec:leftchains}

The pattern used in the solution presented in Section~\ref{sec:haskellproblem}
can be abstracted with the notion of an (heterogeneous) left chain which takes
a parser for a seed, one for a constructor, and one for and argument. The crucial
thing is to make sure \emph{not} to use the parser one is currently defining as
the seed.

\shgrab{hchainl}

We naturally want to include a safe variant of this combinator in our
library. However this definition relies on the ability to simply use
\textit{pure} in case it's not possible to parse an additional
constructor and argument and that is something we simply don't have
access to.

This forces us to find the essence of \parser{rest}, the auxiliary definition
used in \parser{hchainl}: its first argument is not just a value, it is a
\AR{Success} upon which it builds until it can't anymore and simply returns.
We define \AF{schainl} according to this analysis: it is a bare bones version
of \textit{hchainl}'s \textit{rest} where \textit{con} and \textit{arg} have
already been replaced by a single \textit{con} function.

\agrab{Text/Parser/Combinators}{schainl}

A key thing to notice is that we build a list of \AR{Success}es at the
\emph{same} index as the input \AR{Success} and \AR{Parser} which will make
this combinator compositional (as opposed to \parser{some} defined in
Section~\ref{parser:some}). This as a cost in terms of the readability of
the definition of \AF{schainl}. But ultimately all of this complexity only
shows up in the implementation of our library: the end user can blissfully
ignore these details.

From this definition we can derive \AF{iterate} which takes a parser for
a seed and a parser for a function and kickstarts a call to \AF{schainl}
on the result of the parser for the seed.

\agrab{Text/Parser/Combinators}{iterate}

Finally, \AF{hchainl} can be implemented using \AF{iterate}, the applicative
structure of \AR{Parser} and some of the properties of \BOX{}.

\agrab{Text/Parser/Combinators}{hchainl}

As we have mentioned when defining \AF{schainl}, the combinator \AF{hchainl}
we have just implemented does not expect fully-defined parsers as arguments.
As a consequence it can be used inside a fixpoint construction. Both the parser
for the constructor and the one for its \AB{B} argument are guarded whilst the
one for the \AB{A} seed is not. This means that trying to define a left-recursive
grammar by immediately using a recursive substructure on the left is now a \emph{type
error}. But it still possible to have some on the right or after having consumed
at least one character (typically an opening parenthesis, cf. the \type{Expr}
example in Section~\ref{parser:expr}).

\section{Fully Worked-Out Example}\label{example:expr}

From \AF{hchainl}, one can derive \AF{chainl1} which is not heterogeneous
and uses the same parser for the seed and the constructors' arguments.
This combinator together with the idea of precedence mentioned in Section~\ref{parser:expr}
is typically used to implement left-recursive grammars. Looking up the
documentation of the \texttt{parsec} library on hackage~\cite{doc:parsec}
we can find a fine example: an extension of our early arithmetic language
(corresponding grammar on the right hand side).

\noindent
\begin{minipage}{0.45\textwidth}
  \shgrab{example}
\end{minipage}
\begin{minipage}{0.55\textwidth}
  \begin{tabular}{lllll}
    Expr   & := & Term   & | & Term AddOp Expr \\
    Term   & := & Factor & | & Factor MulOp Term \\
    Factor & := & <Int>  & | & '(' Expr ')' \\
    AddOp  & := & '+' & | & '-' \\
    MulOp  & := & '*' & | & '/' \\
  \end{tabular}
\end{minipage}

One important thing to note here is that in the end we not only get a
parser for the \type{expr}essions but also each one of the intermediate
categories \type{term} and \type{factor}. Luckily, our library lets us
take fixpoints of any sized types we may fancy. As such, we can define
a sized record of parsers for each one of the syntactic categories:

\agrab{Text/Parser/Examples/Expr}{language}

Here, unlike the Haskell example, we decide to be painfully explicit
about the syntactic categories we are considering: we mutually define
three inductive types representing \emph{left-associated} arithmetic
expressions.

\begin{minipage}{0.33\textwidth}
  \agrab{Text/Parser/Examples/Expr}{expr}
\end{minipage}
\begin{minipage}{0.35\textwidth}
  \agrab{Text/Parser/Examples/Expr}{term}
\end{minipage}
\begin{minipage}{0.20\textwidth}
  \agrab{Text/Parser/Examples/Expr}{factor}
\end{minipage}

The definition of the parser itself is then basically the same as the
Haskell one. Contrary to a somewhat popular belief, working in a
dependently-typed language does not force us to add any type annotation
except for the top-level one.

\agrab{Text/Parser/Examples/Expr}{parser}

Although quite close to the Haskell version, We can notice four minor changes:

Firstly, the intermediate parsers need to be declared before being used which effectively
reverses the order in which they are spelt out.

Secondly, the recursive calls
are now explicit: in the definition of \type{factor}, \AF{expr} is \AF{map}ped
under the \BOX{} to project the recursive call to the \AD{Expr} \AR{Parser} out
of \AR{Language}.

Thirdly, we use \AF{hchainl} instead of \AF{chainl1} because
breaking the grammar into three distinct categories leads us to parsing
\emph{h}eterogeneous left chains.

Fourthly, we have to insert calls to \AF{box}
to lift \AR{Parser}s into boxed ones whenever the added guarantee that the call
will be guarded is of no use to us. This last point however does not stand in
Coq nor Idris which have a mechanism to declare implicit coercions and where the
typechecker can insert these calls to \AF{box} automatically for us.

\section{More Power: Switching to other Representations}

We have effectively managed to take Haskell's successful approach to defining
a Domain Specific Language of parser combinators and impose type constraints
which make it safe to use in a total setting. All of which we have done whilst
keeping the concision and expressivity of the original libraries. A natural
next question would be the speed and efficiency of such a library.

Although we have been using a concrete type for \AR{Parser} throughout
this article, our library actually implements a more general one. It uses
Agda's instance arguments throughout thus letting the user pick the
representation they like best.

Firstly, there is nothing special about vectors of characters as an input
type: any sized input off of which one can peel characters one at a time
would do. Users may instead use Haskell's \type{Text} packaged together
with an irrelevant proof that the given text has the right length and a
binding for \textit{uncons}. This should lead to a more efficient memory
representation of the text being analysed.

Secondly, there is no reason to limit ourselves to \type{Char} as the
unit of information to be processed. Near all of our combinators are
fully polymorphic over the kind of tokens they can deal with. To run
the parser, the user will have to define an appropriate tokenizer for
their use case. The library provides a trivial one for \AD{Char}.

Thirdly, there is no reason to force the user to get back a \AD{List}
of successes: any Functor which is both a Monad and an Alternative
will do. This means in particular that a user may for instance instrument
a parser with a logging ability to be able to return good error messages,
have a (re)configurable grammar using a \type{Reader} transformer or use
a \type{Maybe} type if they want to make explicit the fact that their
grammar is unambiguous.

\section{Related Work}

This work is based on Hutton and Meijer's influential functional pearl~\citeyear{hutton1998monadic}
which builds on Walder's insight that exception handling and backtracking can be
realised using a list of successes~\citeyear{wadler1985replace}. Similar Domain
Specific Languages have been implemented in various functional languages such as
Scala~\cite{moors2008parser} or, perhaps more interestingly for us, Rust~\cite{couprie2015nom}
where the added type-level information about ownership can help implement a
guaranteed zero-copy parser.

\subsection{Total Parser Combinators}

When it comes to total programming languages, Danielsson's library~\citeyear{danielsson2010total}
is to our knowledge the only attempt so far at defining a library of total
parser combinators in a dependently-typed host language. He reifies recursive
grammars as values of a mixed inductive-coinductive type and tracks at the
type level whether a sub-grammar accepts the empty word and, as a consequence,
whether one can meaningfully take its fixpoint.

The reified approach allows him to define a grammar's semantics in terms
of multisets of words and prove sound a variant of Brzozowski derivatives~\citeyear{brzozowski1964derivatives}
as well as study the equational theory of parsers. The current
implementation, based on the Brzozowski derivatives, is however of complexity
at least exponential in the size of the input.

Our approach, although not able to tackle certification like Danielsson's,
is however more lightweight. Using only strong induction on the natural
numbers, it is compatible with languages a lot less powerful than Agda.
Indeed there is no need for good support for mixed induction and coinduction
in the host language. And although we do rely on enforcing invariants
at the type-level, one could mimic these in languages with even weaker type
systems by defining an \emph{abstract} \BOX{} and only providing the user with
our set of combinators which is guaranteed to be safe.

\subsection{Certified Parsing}

Ambitious projects such as CompCert~\cite{leroy2012compcert} providing
the user with an ever more certified toolchain tend to bring to light
the lack of proven-correct options for very practical concerns such as
parsing. Jourdan, Pottier and Leroy's work~\cite{jourdan2012validating}
fills that gap by certifying the output of an untrusted parser generator
for LR(1) grammars. This approach serves a different purpose than ours:
parser combinators libraries are great for rapid prototyping and small,
re-configurable parsers for non-critical applications.

Bernardy and Jansson have implemented in Agda a fully-certified generalisation
of Valiant's algorithm~\cite{bernardy2016certified} by deriving it from
its specification. This algorithm gives the best asymptotic bounds on
context-free grammar, that is the \type{Applicative} subset tackled by
parser combinators.

\section{Conclusion and Future Work}

Starting from the definition of ``parsers for things as functions from
strings to lists of strings and things'' common in Haskell, we have been
able to (re)define versatile combinators. However the type system was
completely unable to root out some badly-behaved programs, namely the
ones taking the fixpoint of a grammar accepting the empty word or non
well-founded left recursive grammars. Wanting to use a total programming
language, this led us to a radical solution: rejecting all the parsers
accepting the empty word. Luckily, it was still possible to recover a
notion of ``potentially failing'' sub-parses via a \textit{bind}-like
combinator as well as defining combinators for left chains. Finally we
saw that this yielded a perfectly safe and only barely more verbose set
of total parser combinators.

In the process of describing our library we have introduced a set of
type-level combinators for manipulating indexed types and defining
values by strong induction. If we want to provide our users with the
tools to modularly prove some of the properties of their grammars, we
need to come up with proof combinators corresponding to the value ones.
As far as we know this is still an open problem.
